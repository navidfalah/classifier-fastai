# -*- coding: utf-8 -*-
"""image_classification_fastai.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Lrs4Va3ocdysVxXneVHjAys_sZgy6iQS
"""

from fastai.vision.all import *

path = untar_data(URLs.PETS)

path.ls()

(path/"images").ls()

fname = (path/"images").ls()[0]

type(fname), fname.name, dir(fname)

re.findall(r'(.+)_\d+.jpg$', fname.name)

pets = DataBlock(blocks=(ImageBlock, CategoryBlock),
                 get_items=get_image_files,
                 splitter=RandomSplitter(seed=42),
                 get_y=using_attr(RegexLabeller(r'(.+)_\d+.jpg$'), 'name'),
                 item_tfms=Resize(460),
                 batch_tfms=aug_transforms(size=224, min_scale=0.75))

dls = pets.dataloaders(path/"images")

#### befor training your data always check the data

dls.show_batch(nrows=1, ncols=4)

class_labels = dls.vocab
class_labels

### train the model asap because might not need lot of data and data should train the model and change the loss

learn = cnn_learner(dls, resnet34, metrics=error_rate)
learn.fine_tune(2)

### get a batch fron real data

x, y = dls.one_batch()
x.shape, y.shape

y

preds, _ = learn.get_preds(dl=[(x, y)])
preds[0]

len(preds[0]), preds[0].sum()

acts = torch.randn((6, 2))*2
acts

acts.sigmoid()

(acts[:, 0]-acts[:, 1]).sigmoid()

def softmax(acts): return torch.exp(acts)/torch.exp(acts).sum(dim=1, keepdim=True)

sm_acts = softmax(acts)
sm_acts

def mnist_loss(inputs, targets):
    inputs = inputs.sigmoid()
    return torch.where(targets==1, inputs, 1-inputs).log().mean()

targ = tensor([0, 1, 0, 1, 1, 0])

sm_acts

idx = range(6)
sm_acts[idx, targ]

from torch import nn

loss_func = nn.CrossEntropyLoss()

loss_func(acts, targ)

F.cross_entropy(acts, targ)

nn.CrossEntropyLoss(reduction='none')(acts, targ)

interp = ClassificationInterpretation.from_learner(learn)
interp.plot_confusion_matrix(figsize=(12, 12), dpi=60)

interp.most_confused(min_val=5)

learn = cnn_learner(dls, resnet34, metrics=error_rate)
learn.fine_tune(1, base_lr=0.1)

learn = cnn_learner(dls, resnet34, metrics=error_rate)
suggested_lrs = learn.lr_find()      # Capture the single returned value
lr_min = suggested_lrs.valley        # Access the desired learning rate from the named tuple

print(f'min: {lr_min}')

learn = cnn_learner(dls, resnet34, metrics=error_rate)
learn.fine_tune(1, base_lr=lr_min)

learn = cnn_learner(dls, resnet34, metrics=error_rate)
learn.fit_one_cycle(3, lr_max=lr_min)

learn.unfreeze()

learn.lr_find()

learn.fit_one_cycle(3, lr_max=1e-5)

learn = cnn_learner(dls, resnet34, metrics=error_rate)
learn.fit_one_cycle(3, lr_max=3e-3)
learn.unfreeze()
learn.fit_one_cycle(12, lr_max=slice(1e-6, 1e-4))

learn.recorder.plot_loss()

from fastai2.callback.fp16 import *

learn = cnn_learner(dls, resnet34, metrics=error_rate).to_fp16()
learn.fine_tune(6, freeze_epochs=3)

